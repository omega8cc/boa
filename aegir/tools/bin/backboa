#!/bin/bash

###
### Acknowledgements
###
### Thomas Sileo @ https://thomassileo.name
### Original recipe: http://bit.ly/1QX462w
###
### Extended by Barracuda Team for BOA project
###
### See also:
### http://www.nongnu.org/duplicity/index.html
###

PATH=/usr/local/bin:/usr/local/sbin:/opt/local/bin:/usr/bin:/usr/sbin:/bin:/sbin
SHELL=/bin/bash

_DUPLICITY_VRN="0.7.06"
_BOTO_VRN="2.39.0-fix-python-2.7.9"
_LOGPTH="/var/xdrago/log"
_NOW=$(date +%y%m%d-%H%M 2>&1)
_NOW=${_NOW//[^0-9-]/}
_DOW=$(date +%u 2>&1)
_DOW=${_DOW//[^1-7]/}
_DOM=$(date +%e 2>&1)
_DOM=${_DOM//[^0-9]/}
_HST=$(uname -n 2>&1)
_HST=${_HST//[^a-zA-Z0-9-.]/}
_HST=$(echo -n ${_HST} | tr A-Z a-z 2>&1)

crlGet="-L --max-redirs 10 -k -s --retry 10 --retry-delay 5 -A iCab"

check_root() {
  if [ `whoami` = "root" ]; then
    ionice -c2 -n7 -p $$
    renice 19 -p $$
    chmod a+w /dev/null
    if [ ! -e "/dev/fd" ]; then
      if [ -e "/proc/self/fd" ]; then
        rm -rf /dev/fd
        ln -s /proc/self/fd /dev/fd
      fi
    fi
  else
    echo "ERROR: This script should be ran as a root user"
    exit 1
  fi
  _DF_TEST=$(df -kTh / -l \
    | grep '/' \
    | sed 's/\%//g' \
    | awk '{print $6}' 2> /dev/null)
  _DF_TEST=${_DF_TEST//[^0-9]/}
  if [ ! -z "${_DF_TEST}" ] && [ "${_DF_TEST}" -gt "90" ]; then
    echo "ERROR: Your disk space is almost full !!! ${_DF_TEST}/100"
    echo "ERROR: We can not proceed until it is below 90/100"
    exit 1
  fi
}
check_root

find_fast_mirror() {
  isNetc=$(which netcat 2>&1)
  if [ ! -x "${isNetc}" ] || [ -z "${isNetc}" ]; then
    rm -f /etc/apt/sources.list.d/openssl.list
    apt-get update -qq &> /dev/null
    apt-get install netcat -fuy --force-yes --reinstall &> /dev/null
    sleep 3
  fi
  ffMirr=$(which ffmirror 2>&1)
  if [ -x "${ffMirr}" ]; then
    ffList="/var/backups/boa-mirrors.txt"
    mkdir -p /var/backups
    if [ ! -e "${ffList}" ]; then
      echo "jp.files.aegir.cc"  > ${ffList}
      echo "nl.files.aegir.cc" >> ${ffList}
      echo "uk.files.aegir.cc" >> ${ffList}
      echo "us.files.aegir.cc" >> ${ffList}
    fi
    if [ -e "${ffList}" ]; then
      _CHECK_MIRROR=$(bash ${ffMirr} < ${ffList} 2>&1)
      _USE_MIR="${_CHECK_MIRROR}"
      [[ "${_USE_MIR}" =~ "printf" ]] && _USE_MIR="files.aegir.cc"
    else
      _USE_MIR="files.aegir.cc"
    fi
  else
    _USE_MIR="files.aegir.cc"
  fi
  if ! netcat -w 10 -z "${_USE_MIR}" 80; then
    echo "INFO: The mirror ${_USE_MIR} doesn't respond, let's try default"
    _USE_MIR="files.aegir.cc"
  fi
  urlDev="http://${_USE_MIR}/dev"
  urlHmr="http://${_USE_MIR}/versions/master/aegir"
}

install() {
  _DUPLICITY_ITD=$(duplicity --version 2>&1 \
    | tr -d "\n" \
    | cut -d" " -f2 \
    | awk '{ print $1}' 2>&1)
  if [ "${_DUPLICITY_ITD}" = "${_DUPLICITY_VRN}" ] \
    && [ -e "${_LOGPTH}/boto-${_BOTO_VRN}.log" ]; then
    echo "Latest duplicity version ${_DUPLICITY_VRN} already installed"
  else
    echo "Installing duplicity dependencies..."
    cd
    find_fast_mirror
    rm -f /etc/apt/sources.list.d/openssl.list
    apt-get clean -qq                    &> /dev/null
    apt-get update -qq                   &> /dev/null
    aptitude purge duplicity -y          &> /dev/null
    rm -rf /usr/local/lib/python2.7/dist-packages/boto*
    rm -rf /usr/local/lib/python2.6/dist-packages/boto*
    rm -rf /usr/local/lib/python2.7/dist-packages/duplicity*
    rm -rf /usr/local/lib/python2.6/dist-packages/duplicity*
    apt-get install librsync-dev -y      &> /dev/null
    apt-get install python-dev -y        &> /dev/null
    apt-get install python-lockfile -y   &> /dev/null
    apt-get install python-setuptools -y &> /dev/null
    apt-get install s3cmd -y             &> /dev/null
    mkdir -p /var/opt
    rm -rf /var/opt/{boto*,duplicity*}
    cd /var/opt
    fe="boto-${_BOTO_VRN}.tar.gz"
    curl ${crlGet} "${urlDev}/src/${fe}" -o ${fe}
    tar xzf ${fe} &> /dev/null
    cd /var/opt/boto-${_BOTO_VRN}
    python setup.py install &> /dev/null
    echo ${_BOTO_VRN} > ${_LOGPTH}/boto-${_BOTO_VRN}.log
    echo "Building duplicity version ${_DUPLICITY_VRN} from sources..."
    cd /var/opt
    fe="duplicity-${_DUPLICITY_VRN}.tar.gz"
    curl ${crlGet} "${urlDev}/src/${fe}" -o ${fe}
    tar xzf ${fe} &> /dev/null
    cd /var/opt/duplicity-${_DUPLICITY_VRN}
    python setup.py build &> /dev/null
    python setup.py install &> /dev/null
    cd
    rm -rf /var/opt/{boto*,duplicity*}
    echo "Installation complete!"
  fi
}

check_boto() {
  if [ ! -e "${_LOGPTH}/boto-${_BOTO_VRN}.log" ]; then
    echo "Upgrade to boto ${_BOTO_VRN} required..."
    install
  fi
}

if [ `ps aux | grep -v "grep" | grep --count "duplicity"` -gt "0" ]; then
  echo "The duplicity backup is already running!"
  echo "Active duplicity process detected..."
  exit 1
fi

if [ -e "/root/.barracuda.cnf" ]; then
  source /root/.barracuda.cnf
fi

if [ -z "${_AWS_KEY}" ] || [ -z "${_AWS_SEC}" ] || [ -z "${_AWS_PWD}" ]; then
  echo "

  CONFIGURATION REQUIRED!

  Add listed below eight (8) lines to your /root/.barracuda.cnf file.
  Required lines are marked with [R] and optional with [O]:

    _AWS_KEY='Your AWS Access Key ID'     ### [R] From your AWS S3 settings
    _AWS_SEC='Your AWS Secret Access Key' ### [R] From your AWS S3 settings
    _AWS_PWD='Your Secret Password'       ### [R] Generate with 'openssl rand -base64 32'
    _AWS_REG='Your AWS Region ID'         ### [R] By default 'us-east-1'
    _AWS_TTL='Your Backup Rotation'       ### [O] By default '30D'
    _AWS_FLC='Your Backup Full Cycle'     ### [O] By default '7D'
    _AWS_VLV='Your Backup Log Verbosity'  ### [O] By default '1'
    _AWS_PRG='Your Backup Progress'       ### [O] By default 'NO' -- can be YES/NO
    _AWS_EXB='Exclude Drush Archives'     ### [O] By default 'NO' -- can be YES/NO

    Supported values to use as _AWS_REG:

      us-east-1
      us-west-2
      us-west-1
      eu-west-1
      eu-central-1
      ap-southeast-1
      ap-southeast-2
      ap-northeast-1
      ap-northeast-2
      sa-east-1

    Source: http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region

    You have to use S3 Console at https://console.aws.amazon.com/s3/home
    (before attempting to run initial backup!) to create S3 bucket in the
    desired region with correct name as shown below:

      daily.boa.${_HST}

    While duplicity should be able to create new bucket on demand, in practice
    it almost never works due to typical delays between various AWS regions.

    Please run: 'backboa test' to make sure that the connection works.

  "
  exit 1
fi

if [ -z "${_AWS_REG}" ]; then
  _AWS_REG="us-east-1"
fi

if [ "${_AWS_REG}" = "us-east-1" ] \
  || [ "${_AWS_REG}" = "us-west-2" ] \
  || [ "${_AWS_REG}" = "us-west-1" ]; then
  _GOOD_AWS_REG=YES
elif [ "${_AWS_REG}" = "eu-west-1" ] \
  || [ "${_AWS_REG}" = "eu-central-1" ] \
  || [ "${_AWS_REG}" = "ap-southeast-1" ]; then
  _GOOD_AWS_REG=YES
elif [ "${_AWS_REG}" = "ap-southeast-2" ] \
  || [ "${_AWS_REG}" = "ap-northeast-1" ] \
  || [ "${_AWS_REG}" = "ap-northeast-2" ] \
  || [ "${_AWS_REG}" = "sa-east-1" ]; then
  _GOOD_AWS_REG=YES
else
  _AWS_REG="us-east-1"
fi

if [ "${_AWS_REG}" = "us-east-1" ]; then
  _AWS_URL="s3-external-1.amazonaws.com"
else
  _AWS_URL="s3-${_AWS_REG}.amazonaws.com"
fi

_AWS_TTL=${_AWS_TTL//[^A-Z0-9]/}
if [ -z "${_AWS_TTL}" ]; then
  _AWS_TTL="30D"
fi

_AWS_FLC=${_AWS_FLC//[^A-Z0-9]/}
if [ -z "${_AWS_FLC}" ]; then
  _AWS_FLC="7D"
fi

_AWS_VLV=${_AWS_VLV//[^0-9]/}
if [ -z "${_AWS_VLV}" ]; then
  _AWS_VLV="1"
fi

if [ ! -z "${_AWS_EXB}" ] && [ "${_AWS_EXB}" = "YES" ]; then
  EXCLUDE="--exclude /data/conf/arch --exclude /data/disk/*/backups/*.tar.gz"
else
  EXCLUDE="--exclude /data/conf/arch"
fi

_AWS_PRG=${_AWS_PRG//[^A-Z]/}
_AWS_OPX="--s3-use-new-style"

export AWS_ACCESS_KEY_ID="${_AWS_KEY}"
export AWS_SECRET_ACCESS_KEY="${_AWS_SEC}"
export PASSPHRASE="${_AWS_PWD}"

SOURCE="/etc /var/aegir /var/www /home /data"
BUCKET="daily.boa.${_HST}"
TARGET="s3://${_AWS_URL}/${BUCKET}"
LOGFILE="${_LOGPTH}/${BUCKET}.log"

backup_prepare() {
  if [ ! -z "${_AWS_PRG}" ] && [ "${_AWS_PRG}" = "YES" ]; then
    _AWS_OPX="--s3-use-new-style --progress"
  fi
  INCLUDE=""
  for CDIR in ${SOURCE}; do
    TMP=" --include  ${CDIR}"
    INCLUDE="${INCLUDE}${TMP}"
  done
  if [ -e "/root/.cache/duplicity" ]; then
    CacheTest=$(find /root/.cache/duplicity/* \
      -maxdepth 1 \
      -mindepth 1 \
      -type f \
      | sort 2>&1)
    if [[ "${CacheTest}" =~ "No such file or directory" ]] \
      || [ -z "${CacheTest}" ]; then
      _DO_CLEANUP=NO
    else
      _DO_CLEANUP=YES
    fi
  fi
}

monthly_cleanup() {
  if [ -e "${_LOGPTH}/${BUCKET}.randomize.cleanup.log" ]; then
    _RCL=$(cat ${_LOGPTH}/${BUCKET}.randomize.cleanup.log 2>&1)
    _RCL=$(echo -n ${_RCL} | tr -d "\n" 2>&1)
    _RCL=${_RCL//[^1-5]/}
  else
    _RCL=$((RANDOM%5+1))
    _RCL=${_RCL//[^1-5]/}
    echo ${_RCL} > ${_LOGPTH}/${BUCKET}.randomize.cleanup.log
  fi
  if [ -e "${_LOGPTH}/${BUCKET}.archive.log" ] \
    && [ ! -e "/root/.skip_duplicity_monthly_cleanup.cnf" ] \
    && [ "${_DOM}" = "${_RCL}" ] \
    && [ "${_DO_CLEANUP}" = "YES" ]; then
    if [ -e "/root/.randomize_duplicity_full_backup_day.cnf" ]; then
      n=$((RANDOM%10800+8))
      echo "Waiting $n seconds on `date` before running cleanup --force" > ${LOGFILE}
      sleep $n
    fi
    echo "Running cleanup --force on `date`" >> ${LOGFILE}
    duplicity -v ${_AWS_VLV} cleanup --force ${_AWS_OPX} ${TARGET}
    rm -f ${_LOGPTH}/${BUCKET}.randomize.full.log
    rm -f ${_LOGPTH}/${BUCKET}.randomize.cleanup.log
  fi
}

randomize_full() {
  if [ -e "/root/.randomize_duplicity_full_backup_day.cnf" ]; then
    if [ -e "${_LOGPTH}/${BUCKET}.randomize.full.log" ]; then
      _RDW=$(cat ${_LOGPTH}/${BUCKET}.randomize.full.log 2>&1)
      _RDW=$(echo -n ${_RDW} | tr -d "\n" 2>&1)
      _RDW=${_RDW//[^1-7]/}
      _MODE="incr"
    else
      _RDW=$((RANDOM%7+1))
      _RDW=${_RDW//[^1-7]/}
      _MODE="full"
      echo ${_RDW} > ${_LOGPTH}/${BUCKET}.randomize.full.log
    fi
  else
    _RDW=7
  fi
}

set_mode() {
  if [ "${_DOW}" = "${_RDW}" ] && [ "${_AWS_FLC}" = "7D" ]; then
    if [ ! -e "/root/.randomize_duplicity_full_backup_day.cnf" ]; then
      _MODE="full"
      _AWS_FLC="1M"
    fi
  else
    if [ -e "${_LOGPTH}/${BUCKET}.archive.log" ] \
      && [ "${_DO_CLEANUP}" = "YES" ]; then
      _MODE="incr"
    else
      _MODE="full"
    fi
  fi
}

run_backup() {
  if [ -e "/root/.randomize_duplicity_full_backup_day.cnf" ]; then
    n=$((RANDOM%10800+8))
    echo "Waiting $n seconds on `date` before running restore home 7D tmp/home" >> ${LOGFILE}
    sleep $n
    restore home 7D tmp/home >> ${LOGFILE}
    n=$((RANDOM%10800+8))
    echo "Waiting $n seconds on `date` before running ${_MODE} backup" >> ${LOGFILE}
    sleep $n
  fi
  echo "Running ${_MODE} backup on `date`" >> ${LOGFILE}
  duplicity ${_MODE} -v ${_AWS_VLV} --volsize 100 \
    --allow-source-mismatch \
    --full-if-older-than ${_AWS_FLC} \
    --asynchronous-upload ${_AWS_OPX} \
    ${INCLUDE} ${EXCLUDE} --exclude '**' / ${TARGET} >> ${LOGFILE}
}

remove_older_than() {
  if [ -e "/root/.randomize_duplicity_full_backup_day.cnf" ]; then
    n=$((RANDOM%10800+8))
    echo "Waiting $n seconds on `date` before running remove-older-than ${_AWS_TTL}" >> ${LOGFILE}
    sleep $n
  fi
  echo "Running remove-older-than on `date`" >> ${LOGFILE}
  duplicity -v ${_AWS_VLV} remove-older-than ${_AWS_TTL} \
    --force ${_AWS_OPX} ${TARGET} >> ${LOGFILE}
}

collection_status() {
  if [ -e "/root/.randomize_duplicity_full_backup_day.cnf" ]; then
    n=$((RANDOM%10800+8))
    echo "Waiting $n seconds on `date` before running collection-status" >> ${LOGFILE}
    sleep $n
  fi
  echo "Running collection-status on `date`" >> ${LOGFILE}
  duplicity -v ${_AWS_VLV} collection-status ${_AWS_OPX} ${TARGET} >> ${LOGFILE}
}

backup() {
  backup_prepare
  monthly_cleanup
  randomize_full
  set_mode
  run_backup
  if [ -e "${_LOGPTH}/${BUCKET}.archive.log" ] \
    && [ "${_DOW}" = "${_RDW}" ] \
    && [ "${_DO_CLEANUP}" = "YES" ]; then
    remove_older_than
    collection_status
  fi
  if [ -n "${_MY_EMAIL}" ] && [ "${_AWS_VLV}" -lt "3" ]; then
    echo "Sending email report on `date`" >> ${LOGFILE}
    mail -s "Daily backup: ${_MODE} ${_HST} $(date 2>&1)" ${_MY_EMAIL} < ${LOGFILE}
  fi
  cat ${LOGFILE} >> ${_LOGPTH}/${BUCKET}.archive.log
  rm -f ${LOGFILE}
}

conn_test() {
  if [ $# = 1 ]; then
    BUCKET="daily.boa.$1"
    TARGET="s3://${_AWS_URL}/${BUCKET}"
  fi
  echo "Running AWS connection test, please wait..."
  ConnTest=$(duplicity -v 2 cleanup --dry-run \
    --timeout 1 ${_AWS_OPX} ${TARGET} 2>&1)
  ### echo ConnTest is STR ${ConnTest} END
  if [[ "${ConnTest}" =~ "No connection to backend" ]] \
    || [[ "${ConnTest}" =~ "IllegalLocationConstraintException" ]]; then
    echo
    echo "  Sorry, I can't connect to ${TARGET}"
    echo "  Please check if the bucket has expected name: ${BUCKET}"
    echo "  This bucket must already exist in the ${_AWS_REG} AWS region"
    echo "  http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region"
    echo "  Bye"
    echo
    exit 1
  else
    echo "OK, I can connect to ${TARGET}"
  fi
}

status() {
  duplicity -v ${_AWS_VLV} collection-status ${_AWS_OPX} ${TARGET}
}

cleanup() {
  duplicity -v ${_AWS_VLV} cleanup --force ${_AWS_OPX} ${TARGET}
  duplicity -v ${_AWS_VLV} collection-status ${_AWS_OPX} ${TARGET}
}

list() {
  duplicity -v ${_AWS_VLV} list-current-files ${_AWS_OPX} ${TARGET}
}

restore() {
  if [ $# = 2 ]; then
    duplicity -v ${_AWS_VLV} restore \
      --file-to-restore $1 ${_AWS_OPX} ${TARGET} $2
  else
    duplicity -v ${_AWS_VLV} restore \
      --file-to-restore $1 --time $2 ${_AWS_OPX} ${TARGET} $3
  fi
}

retrieve() {
  if [ $# = 3 ]; then
    BUCKET="daily.boa.$3"
    TARGET="s3://${_AWS_URL}/${BUCKET}"
    duplicity restore --file-to-restore $1 ${_AWS_OPX} ${TARGET} $2
  elif [ $# = 4 ]; then
    BUCKET="daily.boa.$4"
    TARGET="s3://${_AWS_URL}/${BUCKET}"
    duplicity restore --file-to-restore $1 --time $2 ${_AWS_OPX} ${TARGET} $3
  fi
}

if [ "$1" = "backup" ]; then
  if test -f /var/run/${_HST}_backup.pid ; then
    touch ${_LOGPTH}/wait_${_HST}_backup.log
    echo "The duplicity backup is running already?"
    echo "Existing /var/run/${_HST}_backup.pid found..."
    echo "But no active duplicity process detected..."
    exit 1
  else
    touch /var/run/${_HST}_backup.pid
    echo "The duplicity backup is starting now..."
    check_boto
    backup
    echo "The duplicity backup is complete!"
    touch ${_LOGPTH}/run_${_HST}_backup.log
    rm -f /var/run/${_HST}_backup.pid
  fi
elif [ "$1" = "install" ]; then
  install
elif [ "$1" = "cleanup" ]; then
  cleanup
elif [ "$1" = "list" ]; then
  list
elif [ "$1" = "restore" ]; then
  if [ $# = 3 ]; then
    restore $2 $3
  else
    restore $2 $3 $4
  fi
elif [ "$1" = "retrieve" ]; then
  if [ $# = 4 ]; then
    retrieve $2 $3 $4
  elif [ $# = 5 ]; then
    retrieve $2 $3 $4 $5
  else
    echo "You have to specify also hostname of the backed up system"
    exit 1
  fi
elif [ "$1" = "status" ]; then
  check_boto
  status
elif [ "$1" = "test" ]; then
  conn_test
else
  echo "

  INSTALLATION:

  $ backboa install

  USAGE:

  $ backboa backup
  $ backboa cleanup
  $ backboa list
  $ backboa status
  $ backboa test
  $ backboa restore file [time] destination
  $ backboa retrieve file [time] destination hostname

  RESTORE EXAMPLES:

  Note: Be careful while restoring not to prepend a slash to the path!

  Restoring a single file to tmp/
  $ backboa restore data/disk/o1/backups/foo.tar.gz tmp/foo.tar.gz

  Restoring an older version of a directory to tmp/ - interval or full date
  $ backboa restore data/disk/o1/backups 7D8h8s tmp/backups
  $ backboa restore data/disk/o1/backups 2014/11/11 tmp/backups

  Restoring data on a different server
  $ backboa retrieve data/disk/o1/backups/foo.tar.gz tmp/foo.tar.gz srv.foo.bar
  $ backboa retrieve data/disk/o1/backups 2014/11/11 tmp/backups srv.foo.bar

  Note: The srv.foo.bar is a hostname of the BOA system backed up before.
        In the 'retrieve' mode it will use the _AWS_* variables configured
        in the current system /root/.barracuda.cnf file - so make sure to edit
        this file to set/replace temporarily all four required _AWS_* variables
        used originally on the host you are retrieving data from! You should
        keep them secret and manage in your offline password manager app.

  "
  exit 1
fi

export AWS_ACCESS_KEY_ID=
export AWS_SECRET_ACCESS_KEY=
export PASSPHRASE=

exit 0
